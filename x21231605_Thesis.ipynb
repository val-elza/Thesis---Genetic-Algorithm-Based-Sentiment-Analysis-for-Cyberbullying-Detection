{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964e9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from genetic_algorithm import GeneticAlgorithm  # Import the genetic algorithm library\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809fb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8bac6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/valarinemichael/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/valarinemichael/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "433f5538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69249bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/valarinemichael/Desktop/SEM -2 /Thesis/Twitter_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c6f76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4defee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['category'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba36f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "        # Check if text is a string\n",
    "        if isinstance(text, str):\n",
    "            # Convert text to lowercase\n",
    "            text = text.lower()\n",
    "\n",
    "            # Remove URLs\n",
    "            text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "            # Remove numbers\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "            # Remove punctuation\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "            # Tokenization\n",
    "            tokens = text.split()\n",
    "\n",
    "            # Remove stopwords\n",
    "\n",
    "            tokens = [word for word in tokens if word not in stopwords_set]\n",
    "\n",
    "            # Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "            # Join tokens back into a single string\n",
    "            cleaned_text = ' '.join(tokens)\n",
    "\n",
    "            return cleaned_text\n",
    "\n",
    "        # Return empty string if text is not a string\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa91493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b50622e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['clean_text'], inplace=True)\n",
    "df.dropna(subset=['category'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9aafd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text'].values\n",
    "# y = tf.keras.utils.to_categorical(df['category'], num_classes=len(df['category'].unique()))\n",
    "\n",
    "y= df['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88265285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -1.0\n",
       "1    0.0\n",
       "2    1.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "5    0.0\n",
       "6    0.0\n",
       "7    0.0\n",
       "8    1.0\n",
       "9    1.0\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "666ca82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  0.,  1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3f067db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = df['clean_text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaa72070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11504"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in lens if x> 180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c46ff1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102294\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 200000\n",
    "max_text_len = 180\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['clean_text'].values)\n",
    "le = len(tokenizer.word_index) + 1\n",
    "print(le)\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_text'].values)\n",
    "X_DeepLearning = tf.keras.utils.pad_sequences(sequences, maxlen=max_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ca76083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((122229, 180), (122229, 3), (40744, 180), (40744, 3))\n"
     ]
    }
   ],
   "source": [
    "labels = tf.keras.utils.to_categorical(df['category'], num_classes=3)\n",
    "XX_train, XX_test, y_train, y_test = train_test_split(X_DeepLearning , labels, test_size=0.25, random_state=42)\n",
    "print((XX_train.shape, y_train.shape, XX_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20c6e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Activation, Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Flatten, GRU, Conv1D, MaxPooling1D, Bidirectional\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9275b828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162973, 180)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_DeepLearning.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "579faec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 180, 256)          51200000  \n",
      "                                                                 \n",
      " spatial_dropout1d (Spatial  (None, 180, 256)          0         \n",
      " Dropout1D)                                                      \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 600)               1336800   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 600)               0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 600)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                38464     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52575459 (200.56 MB)\n",
      "Trainable params: 52575459 (200.56 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "emb_dim = 256\n",
    "batch_size = 50\n",
    "model_lstm1 = Sequential()\n",
    "model_lstm1.add(tf.keras.Input(shape=(X_DeepLearning.shape[1],)))\n",
    "model_lstm1.add(Embedding(vocabulary_size,emb_dim, input_length=X_DeepLearning.shape[1],) )\n",
    "model_lstm1.add(SpatialDropout1D(0.8))\n",
    "model_lstm1.add(Bidirectional(LSTM(300, dropout=0.5, recurrent_dropout=0.5)))\n",
    "model_lstm1.add(Dropout(0.5))\n",
    "model_lstm1.add(Flatten())\n",
    "model_lstm1.add(Dense(64, activation='relu'))\n",
    "model_lstm1.add(Dropout(0.5))\n",
    "model_lstm1.add(Dense(3, activation='softmax'))\n",
    "model_lstm1.compile(optimizer=tf.optimizers.Adam(),loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model_lstm1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f9c23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(filepath=\"lastm-1-layer-best_model.h5\", save_best_only=True, monitor=\"val_acc\", mode=\"max\", verbose=1)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, verbose=1, mode=\"min\", min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr_callback]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86e9368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "489/489 [==============================] - ETA: 0s - loss: 0.7182 - acc: 0.6985\n",
      "Epoch 1: val_acc improved from -inf to 0.86501, saving model to lastm-1-layer-best_model.h5\n",
      "489/489 [==============================] - 985s 2s/step - loss: 0.7182 - acc: 0.6985 - val_loss: 0.4002 - val_acc: 0.8650 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "489/489 [==============================] - ETA: 0s - loss: 0.4068 - acc: 0.8683\n",
      "Epoch 2: val_acc improved from 0.86501 to 0.89608, saving model to lastm-1-layer-best_model.h5\n",
      "489/489 [==============================] - 1025s 2s/step - loss: 0.4068 - acc: 0.8683 - val_loss: 0.3374 - val_acc: 0.8961 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "489/489 [==============================] - ETA: 0s - loss: 0.3418 - acc: 0.8938\n",
      "Epoch 3: val_acc improved from 0.89608 to 0.90340, saving model to lastm-1-layer-best_model.h5\n",
      "489/489 [==============================] - 1014s 2s/step - loss: 0.3418 - acc: 0.8938 - val_loss: 0.3157 - val_acc: 0.9034 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "489/489 [==============================] - ETA: 0s - loss: 0.3066 - acc: 0.9068\n",
      "Epoch 4: val_acc did not improve from 0.90340\n",
      "489/489 [==============================] - 1052s 2s/step - loss: 0.3066 - acc: 0.9068 - val_loss: 0.3137 - val_acc: 0.9030 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "489/489 [==============================] - ETA: 0s - loss: 0.2815 - acc: 0.9142\n",
      "Epoch 5: val_acc improved from 0.90340 to 0.90453, saving model to lastm-1-layer-best_model.h5\n",
      "489/489 [==============================] - 1060s 2s/step - loss: 0.2815 - acc: 0.9142 - val_loss: 0.3269 - val_acc: 0.9045 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "489/489 [==============================] - ETA: 0s - loss: 0.2618 - acc: 0.9217\n",
      "Epoch 6: val_acc did not improve from 0.90453\n",
      "489/489 [==============================] - 1000s 2s/step - loss: 0.2618 - acc: 0.9217 - val_loss: 0.3305 - val_acc: 0.9041 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "489/489 [==============================] - ETA: 0s - loss: 0.2429 - acc: 0.9276\n",
      "Epoch 7: val_acc did not improve from 0.90453\n",
      "489/489 [==============================] - 1027s 2s/step - loss: 0.2429 - acc: 0.9276 - val_loss: 0.3378 - val_acc: 0.9022 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "489/489 [==============================] - ETA: 0s - loss: 0.2286 - acc: 0.9315\n",
      "Epoch 8: val_acc did not improve from 0.90453\n",
      "489/489 [==============================] - 972s 2s/step - loss: 0.2286 - acc: 0.9315 - val_loss: 0.3488 - val_acc: 0.8981 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "489/489 [==============================] - ETA: 0s - loss: 0.2140 - acc: 0.9359\n",
      "Epoch 9: val_acc did not improve from 0.90453\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "489/489 [==============================] - 945s 2s/step - loss: 0.2140 - acc: 0.9359 - val_loss: 0.3470 - val_acc: 0.8947 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "489/489 [==============================] - ETA: 0s - loss: 0.1963 - acc: 0.9419\n",
      "Epoch 10: val_acc did not improve from 0.90453\n",
      "489/489 [==============================] - 941s 2s/step - loss: 0.1963 - acc: 0.9419 - val_loss: 0.3611 - val_acc: 0.8959 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history_lstm1 = model_lstm1.fit(XX_train, y_train, epochs = epochs, batch_size = 250, validation_data=(XX_test, y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc744186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming XX_train and y_train are your feature and label arrays\n",
    "# Create a tuple of feature and label arrays\n",
    "data_tuple = (XX_train, y_train)\n",
    "\n",
    "# Create a TensorFlow Dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(data_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85eb1095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122229"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c7e33fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices((XX_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac0fd43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_ds.take(1))[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f914d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e4f6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "309d74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text'].values\n",
    "# y = tf.keras.utils.to_categorical(df['category'], num_classes=len(df['category'].unique()))\n",
    "\n",
    "y= df['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9336bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2e41591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afd12e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66956     1.0\n",
       "110057    1.0\n",
       "64039     1.0\n",
       "79604    -1.0\n",
       "59915     1.0\n",
       "         ... \n",
       "119879   -1.0\n",
       "103694    0.0\n",
       "131933   -1.0\n",
       "146868    1.0\n",
       "121958    0.0\n",
       "Name: category, Length: 130378, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d617ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred = logistic_regression.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdb65a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  1., -1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a23cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89c4f2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.879306642123025"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, _pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5250fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b37ffcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f046779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d1d3fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63b4a205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(decision_function_shape=&#x27;ovo&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(decision_function_shape=&#x27;ovo&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(decision_function_shape='ovo')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(decision_function_shape='ovo')\n",
    "svm.fit(X_train_tfidf, y_train.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e7baada",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_prd = svm.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c372be42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8833563429973922"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, svc_prd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7c5e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GeneticAlgorithm:\n",
    "    def __init__(self, population_size, num_features, num_generations):\n",
    "        self.population_size = population_size\n",
    "        self.num_features = num_features\n",
    "        self.num_generations = num_generations\n",
    "        self.population = None\n",
    "        self.fitness_scores = None\n",
    "        self.selected_features = None\n",
    "\n",
    "    def initialize_population(self):\n",
    "        self.population = np.random.randint(2, size=(self.population_size, self.num_features))\n",
    "\n",
    "    def calculate_fitness(self, model, X_train, X_test, y_train, y_test):\n",
    "        self.fitness_scores = []\n",
    "        for individual in self.population:\n",
    "            selected_features = [i for i in range(self.num_features) if individual[i] == 1]\n",
    "            X_train_selected = X_train[:, selected_features]\n",
    "            X_test_selected = X_test[:, selected_features]\n",
    "            model.fit(X_train_selected, y_train)\n",
    "            y_pred = model.predict(X_test_selected)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            self.fitness_scores.append(accuracy)\n",
    "\n",
    "    def selection(self):\n",
    "        tournament_size = 3\n",
    "        selected_indices = []\n",
    "        for _ in range(self.population_size):\n",
    "            tournament_indices = np.random.choice(range(self.population_size), size=tournament_size, replace=False)\n",
    "            tournament_fitness = [self.fitness_scores[i] for i in tournament_indices]\n",
    "            winner_index = tournament_indices[np.argmax(tournament_fitness)]\n",
    "            selected_indices.append(winner_index)\n",
    "        self.population = self.population[selected_indices]\n",
    "\n",
    "    def crossover(self):\n",
    "        offspring = []\n",
    "        for _ in range(self.population_size):\n",
    "            parent_indices = np.random.choice(range(self.population_size), size=2, replace=False)\n",
    "            parent1, parent2 = self.population[parent_indices]\n",
    "            parent1 = parent1.ravel()  # Reshape to 1D array\n",
    "            parent2 = parent2.ravel()  # Reshape to 1D array\n",
    "            crossover_point = np.random.randint(1, self.num_features)\n",
    "            child = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
    "            offspring.append(child)\n",
    "        self.population = np.array(offspring)\n",
    "\n",
    "    def mutation(self, mutation_rate):\n",
    "        for individual in self.population:\n",
    "            for i in range(self.num_features):\n",
    "                if np.random.rand() < mutation_rate:\n",
    "                    individual[i] = 1 - individual[i]\n",
    "\n",
    "    def optimize(self, model, X_train, X_test, y_train, y_test):\n",
    "        self.initialize_population()\n",
    "        for _ in range(self.num_generations):\n",
    "            self.calculate_fitness(model, X_train, X_test, y_train, y_test)\n",
    "            self.selection()\n",
    "            self.crossover()\n",
    "            self.mutation(mutation_rate=0.01)\n",
    "        best_individual_index = np.argmax(self.fitness_scores)\n",
    "        self.selected_features = [i for i in range(self.num_features) if self.population[best_individual_index][i] == 1]\n",
    "        return self.selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e85d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the GeneticAlgorithm class\n",
    "ga = GeneticAlgorithm(population_size=10, num_features=X_train_tfidf.shape[1], num_generations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab9c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the genetic algorithm to optimize feature selection\n",
    "selected_features = ga.optimize(logistic_regression, X_train_tfidf, X_test_tfidf, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8bacd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply feature selection to the training and testing data\n",
    "X_train_selected = X_train_tfidf[:, selected_features]\n",
    "X_test_selected = X_test_tfidf[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- List of models and their names\n",
    "models = [svm_model, logistic_regression_model]\n",
    "model_names = ['SVM', 'Logistic Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef4314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a figure for ROC curve plotting\n",
    "plt.figure(figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9fcf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each model and print evaluation results\n",
    "for i, model in enumerate(models):\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_result = classification_report(y_test, y_pred)  # Change the variable name here\n",
    "    confusion_matrix_result = confusion_matrix(y_test, y_pred)  # Change the variable name here\n",
    "    \n",
    "    # ROC curve calculation\n",
    "    y_pred_prob = model.predict_proba(X_test_selected)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(f\"{model_names[i]} Model:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_result)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, label=f'{model_names[i]} (AUC = {roc_auc:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e053140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels and legend to the plot\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models and their names\n",
    "models = [svm]\n",
    "model_names = ['SVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f455d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store accuracy scores\n",
    "accuracy_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model and store the accuracy score\n",
    "for model in models:\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a20d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
